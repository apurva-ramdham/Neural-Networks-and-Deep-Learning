{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Columbia University\n",
    "### ECBM E4040 Neural Networks and Deep Learning. Fall 2019."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1, Task 4: Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 \n",
    "Cross entropy is a metric that measures the \"distance\" between two distributions, why can it be used in calculating the loss of softmax classifier? \n",
    "\n",
    "   Your answer: **Even though cross entropy is a metric that measures distance between two distributions, the distance can be used to measure how close or how far are we from the ground truth. Cross entropy loss helps us to understand how close our probabilities are for the predicted scores with respect to the output.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 \n",
    "Please first describe the difference between multi-class and binary logistic regression; then describe another possible way to derive a multi-class logistic regression classifier from a binary one; finally, illustrate how they work in a deep learning classification model.\n",
    "\n",
    "   Your answer: **The difference between binary and multi-class logistic regression is that binary logistic regression can only classify two labels wheras multi-class can classify n labels where n is greater than 2. Softmax could be used to extend the classification from binary to multi-class. In a deep-learning classification model, binary logistic regression or multi-class is used at the final layer to get real labels of classification**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "Why is the ReLU activation function used the most often in neural networks for computer vision?\n",
    "\n",
    "   Your answer: **ReLU has two advantages :- 1) Vanishing gradient - In computer vision as the layers increase during backpropogation there's a problem with gradient descent. Since ReLU has no negative gradient, the back-propogation is faster.\n",
    "   2) Sparsity- Sparsity comes into picture when ReLU(a) ,such that a < 0. Sparse representations help for dense layer representations which is often used in computer vision.**\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "**Cross validation** is a technique used to prove the generalization ability of a model and can help you find a robust set of hyperparameters. Please describe the implementation details of **k-fold cross validation**.\n",
    "\n",
    "   Your answer: **Cross-validation is a statistical method used to estimate the skill of a learning model. In k-fold cross validation, the data is first partitioned into k parts of (approximately) equal size, called folds. Next, a sequence of models is trained. The first model is trained using the first fold as the test set, and the remaining folds are used as the training set. The model is built using the data in folds, and then the accuracy is evaluated on first fold. Then another model is built, this time using second fold as the test set and the data in other folds are used for training model. This process is repeated for each fold and accuracy is evaluated at each fold. The average accuracy is sum of accuracies at each fold divided by k. Using this we can check different set of hyperparameter to check the validation accuracy. The hyper-parameters corresponding to the best accuracy model are the best set of hyper-parameters and hence can be used for tuning hyper-parameter.**\n",
    "   \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "Describe your best model in the implementation of the two-layer neural network. Describe your starting point, how you tuned  hyperparameters, which stategies you used to improve the network, show the results of intermediate and the final steps.\n",
    "\n",
    "   Your answer: **The starting point was the test and training accuracies from the model given. Since the accuracies were close to 50, I increased the epoch size to double without disturbing the other parameters to check out the effect on accuracies. The results were quite promising and test validation accuracy was above 50%**\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "(Optional, this question is included in the 10 points bonus) In tSNE, describe the motivation of tuning the parameter and discuss the difference in results you see.\n",
    "    \n",
    "   Your answer: **I tuned the input using softmax and there was quite a observable difference. The time taken by the fine-tuned model was low. The cost dropped to around 0.48 for the model with softmax applied and cost function in general was 0.85. Hence a better minmum cost is obtained by tuning the parameters for tSNE**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
